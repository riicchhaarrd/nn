<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <title>Enhanced Neural Network Curve Approximation with C Export</title>
  <style>
    /* Container for side-by-side layout */
    #container {
      display: flex;
      justify-content: center;
      align-items: flex-start;
      gap: 20px;
      margin: 20px;
    }

    /* Left panel for the canvas */
    #left-panel {}

    /* Right panel for stats and export function */
    #right-panel {
      display: flex;
      flex-direction: column;
      gap: 20px;
      width: 300px;
    }

    #right-panel2 {
      display: flex;
      flex-direction: column;
      gap: 20px;
    }

    /* Styling for stats and export containers */
    #stats,
    #export-container {
      width: 100%;
      font-family: monospace;
      white-space: pre-wrap;
      background: #f9f9f9;
      padding: 10px;
      border: 1px solid #ddd;
      box-sizing: border-box;
    }

    #export-container2 {
      font-family: monospace;
      background: #f9f9f9;
      padding: 10px;
      border: 1px solid #ddd;
      box-sizing: border-box;
    }

    #image-upload-container {
      display: none;
      flex-direction: column;
      gap: 10px;
      margin-top: 10px;
      padding: 10px;
      background: #f0f0f0;
      border: 1px solid #ddd;
    }

    #preview-canvas {
      border: 1px solid #ddd;
    }

    #export {
      height: 430px;
      width: 370px;
      font-family: monospace;
      box-sizing: border-box;
    }

    canvas {
      border: 1px solid #333;
      display: block;
    }

    /* Styling for control panel */
    #control-panel {
      margin-top: 10px;
      padding: 10px;
      background: #f0f0f0;
      border: 1px solid #ddd;
    }

    #network-controls {
      display: grid;
      grid-template-columns: auto auto;
      gap: 8px;
      align-items: center;
      margin-bottom: 10px;
    }

    #function-controls {
      display: grid;
      grid-template-columns: auto auto;
      gap: 8px;
      align-items: center;
      margin-top: 10px;
      padding-top: 10px;
      border-top: 1px solid #ddd;
    }

    select,
    input,
    label {
      font-family: sans-serif;
    }

    input[type="number"] {
      width: 60px;
    }

    button {
      margin-top: 10px;
      padding: 5px 10px;
      background: #4CAF50;
      color: white;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    button:hover {
      background: #45a049;
    }
  </style>
</head>

<body>
  <div id="container">
    <div id="left-panel">
      <canvas id="canvas" width="600" height="400"></canvas>
      <div id="control-panel">
        <div id="network-controls">
          <!-- New controls for vector dimensions -->

          <div style="visibility: hidden;display:none;">
            <label for="input-dim">Input Dim:</label>
            <input type="number" id="input-dim" min="1" max="10" value="2">
            <label for="output-dim">Output Dim:</label>
            <input type="number" id="output-dim" min="1" max="10" value="3">
          </div>
          <label for="hidden-size">Hidden Neurons:</label>
          <input type="number" id="hidden-size" min="1" max="50" value="20">

          <label for="num-layers">Hidden Layers:</label>
          <input type="number" id="num-layers" min="1" max="10" value="4">

          <div style="visibility: hidden;display:none;">

            <label for="learning-rate">Learning Rate:</label>
            <input type="number" id="learning-rate" min="0.0001" max="1" step="0.001" value="0.001">

          </div>
          <label for="resolution-input">Preview resolution:</label>
          <input type="number" id="resolution-input" min="1" value="64">
          <label for="batch-size">Training Steps/Frame:</label>
          <input type="number" id="batch-size" min="1" max="1000" value="10000">

          <label for="activation-function">Activation Function:</label>
          <select id="activation-function">
            <option value="cos">Cosine</option>
            <option value="tanh">Tanh</option>
            <option value="sigmoid">Sigmoid</option>
            <option value="relu">ReLU</option>
            <option value="leakyRelu">Leaky ReLU</option>
            <option value="swish">Swish</option>
            <option value="smoothstep">Smoothstep</option>
            <option value="sin">Sine</option>
            <option value="sin30">Sine Omega 30 (SIREN)</option>
            <option value="mulx">x*(1.-x)*(1.-x)</option>
            <!-- https://arxiv.org/abs/2006.09661 -->
            <option value="gelu">GELU</option>
            <option value="elu">ELU</option>
            <option value="softplus">Softplus</option>
            <option value="linear">Linear</option>
            <option value="rayleigh">Rayleighâ€‘Jeans</option>
            <option value="expdecay">Exponential Decay</option>
          </select>
        </div>

        <div id="function-controls">
          <label for="target-function">Target Function:</label>
          <select id="target-function">
            <option value="image">Image</option>
            <!-- <option value="sine">Sine</option>
                        <option value="polynomial">Polynomial</option>
                        <option value="square">Square Wave</option>
                        <option value="step">Step Function</option>
                        <option value="custom">Custom</option> -->
          </select>

          <label for="custom-function" id="custom-function-label" style="display:none;">Custom
            Function:</label>
          <input type="text" id="custom-function" value="Math.sin(x)" style="display:none;">
        </div>

        <button id="reset-network">Reset Network</button>
        <!-- <button id="pause-training">Pause Training</button> -->
        <button id="pause-training">Resume Training</button>
      </div>

      <!-- Add image upload container -->
      <div id="image-upload-container">
        <p>Upload an image to train on:</p>
        <input type="file" id="image-upload" accept="image/*">
        <div>
          <label for="image-scale">Image scale:</label>
          <input type="range" id="image-scale" min="0.1" max="1.0" step="0.1" value="1.0">
          <span id="scale-value">1.0</span>
        </div>
        <canvas id="preview-canvas" width="200" height="200"></canvas>
        <button id="use-image">Use This Image</button>
      </div>
    </div>
    <div id="right-panel">
      <div id="stats"></div>
    </div>
    <div id="right-panel2">
      <div id="export-container2">
        <strong>Exported C Function:</strong>
        <div>
          <textarea id="export" readonly></textarea>
        </div>
      </div>
    </div>
  </div>

  <script>
    // Helper functions for smoothstep
    function mix(a, b, weightb) { return (1 - weightb) * a + weightb * b; }
    function squared(x) { return x * x; }
    function flip(x) { return 1 - x; }
    function smoothstart(t) { return squared(t); }
    function smoothstop(t) { return flip(squared(flip(t))); }
    function smoothstepFunc(x) {
      let t = (x + 4) / 8;
      t = Math.max(0, Math.min(1, t));
      let result = mix(smoothstart(t), smoothstop(t), t);
      return result * 2 - 1;
    }
    function smoothstepPrime(x) {
      let t = (x + 4) / 8;
      if (t <= 0 || t >= 1) return 0;
      let dtResult = 6 * t * (1 - t);
      return dtResult * (1 / 8) * 2;
    }
    // Process uploaded image to grayscale values
    function processImage(img, scale) {
      const previewCanvas = document.getElementById('preview-canvas');
      const previewCtx = previewCanvas.getContext('2d');

      // Calculate new dimensions based on scale
      const scaledWidth = Math.floor(img.width * scale);
      const scaledHeight = Math.floor(img.height * scale);

      // Resize preview canvas
      previewCanvas.width = scaledWidth;
      previewCanvas.height = scaledHeight;

      // Draw and get pixel data
      previewCtx.drawImage(img, 0, 0, scaledWidth, scaledHeight);
      const imageData = previewCtx.getImageData(0, 0, scaledWidth, scaledHeight);
      const pixels = imageData.data;

      const data = [];
      for (let i = 0; i < pixels.length; i += 4) {
        // Normalize to -1 to 1 range
        data.push([(pixels[i] / 128.0) - 1.0, (pixels[i + 1] / 128.0) - 1.0, (pixels[i + 2] / 128.0) - 1.0]);
      }

      return {
        data: data,
        width: scaledWidth,
        height: scaledHeight
      };
    }

    // Get image value at x,y coordinates for NN training/prediction
    function getImageValue(x, y) {
      // Map the x and y to image coordinate space
      if (!imageData || !imageData.data) return 0;

      // Convert from neural network input space (-PI to PI) to image space (0 to width/height)
      const ix = Math.floor(((x - xMin) / (xMax - xMin)) * imageData.width);
      const iy = Math.floor(((y - yMin) / (yMax - yMin)) * imageData.height);

      // Check bounds
      if (ix < 0 || ix >= imageData.width || iy < 0 || iy >= imageData.height) return 0;

      // Calculate linear index
      const index = iy * imageData.width + ix;

      // Return the normalized grayscale value
      // return imageData.data[index] || 0;
      return imageData.data[index] || [0, 0, 0];
    }

    // Sample points from image for training
    function sampleImagePoint() {
      const x = Math.random() * (xMax - xMin) + xMin;
      const y = Math.random() * (yMax - yMin) + yMin;
      return { x, y, value: getImageValue(x, y) };
    }

    let epoch_ = 0;
    // Neural Network supporting multi-dimensional input/output and multiple hidden layers
    class NeuralNetwork {
      constructor(inputSize, hiddenSize, numHiddenLayers, outputSize, learningRate) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.numHiddenLayers = numHiddenLayers;
        this.outputSize = outputSize;
        this.learningRate = learningRate;
        this.activationFunction = "cos"; // default activation
        this.initializeWeights();
      }

      initializeWeights() {
        epoch_ = 0;

        // Hidden layers weights & biases
        this.weights = [];
        this.biases = [];
        let inDim = this.inputSize;
        for (let l = 0; l < this.numHiddenLayers; l++) {
          let layerWeights = [];
          let layerBiases = [];
          for (let i = 0; i < this.hiddenSize; i++) {
            let neuronWeights = [];
            for (let j = 0; j < inDim; j++) {
              neuronWeights.push(Math.random() * 2 - 1);
            }
            layerWeights.push(neuronWeights);
            layerBiases.push(Math.random() * 2 - 1);
          }
          this.weights.push(layerWeights);
          this.biases.push(layerBiases);
          inDim = this.hiddenSize;
        }
        // Output layer: matrix of dimension outputSize x hiddenSize and biases vector of length outputSize
        this.outputWeights = [];
        for (let i = 0; i < this.outputSize; i++) {
          let row = [];
          for (let j = 0; j < this.hiddenSize; j++) {
            row.push(Math.random() * 2 - 1);
          }
          this.outputWeights.push(row);
        }
        this.outputBias = [];
        for (let i = 0; i < this.outputSize; i++) {
          this.outputBias.push(Math.random() * 2 - 1);
        }
      }

      setActivationFunction(funcName) { this.activationFunction = funcName; }
      setLearningRate(rate) { this.learningRate = rate; }
      setHiddenSize(size) { this.hiddenSize = size; this.initializeWeights(); }
      setNumHiddenLayers(numLayers) { this.numHiddenLayers = numLayers; this.initializeWeights(); }

      // Activation and derivative functions
      activate(x) {
        switch (this.activationFunction) {
          case "sigmoid": return 1 / (1 + Math.exp(-x));
          case "relu": return Math.max(0, x);
          case "leakyRelu": return x > 0 ? x : 0.01 * x;
          case "swish": return x * (1 / (1 + Math.exp(-x)));
          case "smoothstep": return smoothstepFunc(x);
          case "cos": return Math.cos(x);
          case "sin": return Math.sin(x);
          case "sin30": return Math.sin(0.5236 * x);
          case "mulx": return x * (1. - x) * (1. - x);
          case "gelu": return 0.5 * x * (1 + Math.tanh(Math.sqrt(2 / Math.PI) * (x + 0.044715 * Math.pow(x, 3))));
          case "elu": return x >= 0 ? x : (Math.exp(x) - 1);
          case "softplus": return Math.log(1 + Math.exp(x));
          case "linear": return x;
          case "rayleigh": return x > 0 ? x * x : 0;
          case "expdecay": return Math.exp(-x);
          case "tanh":
          default: return Math.tanh(x);
        }
      }

      toFlatArray() {
        let flat = [];
        // Serialize hidden layers
        for (let l = 0; l < this.numHiddenLayers; l++) {
          // Flatten weights for current layer
          for (let i = 0; i < this.hiddenSize; i++) {
            flat.push(...this.weights[l][i]);
          }
          // Flatten biases for current layer
          flat.push(...this.biases[l]);
        }
        // Serialize output layer
        for (let i = 0; i < this.outputSize; i++) {
          flat.push(...this.outputWeights[i]);
        }
        flat.push(...this.outputBias);
        return flat;
      }

      activatePrime(x) {
        switch (this.activationFunction) {
          case "sigmoid": {
            let sig = 1 / (1 + Math.exp(-x));
            return sig * (1 - sig);
          }
          case "relu": return x > 0 ? 1 : 0;
          case "leakyRelu": return x > 0 ? 1 : 0.01;
          case "swish": {
            let sigm = 1 / (1 + Math.exp(-x));
            return sigm + x * sigm * (1 - sigm);
          }
          case "smoothstep": return smoothstepPrime(x);
          case "cos": return -Math.sin(x);
          case "sin": return Math.cos(x);
          case "sin30": return 0.5236 * Math.cos(0.5236 * x);
          case "mulx": return (1. - x) * (1. - 3. * x);
          case "gelu": {
            const k = Math.sqrt(2 / Math.PI);
            const tanhTerm = Math.tanh(k * (x + 0.044715 * Math.pow(x, 3)));
            const left = 0.5 * (1 + tanhTerm);
            const sech2 = 1 - tanhTerm * tanhTerm;
            const right = 0.5 * x * sech2 * k * (1 + 3 * 0.044715 * Math.pow(x, 2));
            return left + right;
          }
          case "elu": return x >= 0 ? 1 : Math.exp(x);
          case "softplus": return 1 / (1 + Math.exp(-x));
          case "linear": return 1;
          case "rayleigh": return x > 0 ? 2 * x : 0;
          case "expdecay": return -Math.exp(-x);
          case "tanh":
          default: return 1 - Math.tanh(x) * Math.tanh(x);
        }
      }

      // Forward pass: x is an array of length inputSize; returns an array of length outputSize.
      forward(x) {
        let current = x.slice();
        this.a = [];
        this.h = [];
        for (let l = 0; l < this.numHiddenLayers; l++) {
          let aLayer = [];
          let hLayer = [];
          for (let i = 0; i < this.hiddenSize; i++) {
            let sum = 0;
            for (let j = 0; j < current.length; j++) {
              sum += current[j] * this.weights[l][i][j];
            }
            sum += this.biases[l][i];
            aLayer.push(sum);
            hLayer.push(this.activate(sum));
          }
          this.a.push(aLayer);
          this.h.push(hLayer);
          current = hLayer;
        }
        let output = [];
        for (let i = 0; i < this.outputSize; i++) {
          let sum = this.outputBias[i];
          for (let j = 0; j < current.length; j++) {
            sum += current[j] * this.outputWeights[i][j];
          }
          output.push(sum);
        }
        return output;
      }

      // Train on one sample: x (array) and target (array) should have proper dimensions.
      train(x, target, learningRate) {
        let output = this.forward(x);
        let errors = [];
        for (let i = 0; i < this.outputSize; i++) {
          errors.push(output[i] - target[i]);
        }

        let lastLayer = this.numHiddenLayers - 1;
        // Update output layer weights and biases
        for (let i = 0; i < this.outputSize; i++) {
          for (let j = 0; j < this.hiddenSize; j++) {
            let grad = errors[i] * this.h[lastLayer][j];
            this.outputWeights[i][j] -= learningRate * grad;
          }
          this.outputBias[i] -= learningRate * errors[i];
        }

        // Backpropagation through hidden layers
        let deltas = [];
        let delta = [];
        for (let j = 0; j < this.hiddenSize; j++) {
          let sum = 0;
          for (let i = 0; i < this.outputSize; i++) {
            sum += errors[i] * this.outputWeights[i][j];
          }
          delta.push(sum * this.activatePrime(this.a[lastLayer][j]));
        }
        deltas[lastLayer] = delta;

        for (let l = this.numHiddenLayers - 2; l >= 0; l--) {
          let deltaNext = deltas[l + 1];
          let deltaCurrent = [];
          for (let i = 0; i < this.hiddenSize; i++) {
            let sum = 0;
            for (let k = 0; k < this.hiddenSize; k++) {
              sum += deltaNext[k] * this.weights[l + 1][k][i];
            }
            deltaCurrent.push(sum * this.activatePrime(this.a[l][i]));
          }
          deltas[l] = deltaCurrent;
        }

        for (let l = 0; l < this.numHiddenLayers; l++) {
          let layerInput = (l === 0) ? x : this.h[l - 1];
          for (let i = 0; i < this.hiddenSize; i++) {
            for (let j = 0; j < layerInput.length; j++) {
              this.weights[l][i][j] -= learningRate * deltas[l][i] * layerInput[j];
            }
            this.biases[l][i] -= learningRate * deltas[l][i];
          }
        }
      }
    }

    // Coordinate transforms for canvas drawing
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const width = canvas.width, height = canvas.height;
    const xMin = -Math.PI, xMax = Math.PI, yMin = -1.5, yMax = 1.5;
    // const xMin = -Math.PI, xMax = Math.PI, yMin = -Math.PI, yMax = Math.PI;
    // const xMin = -Math.PI * 2, xMax = Math.PI * 2, yMin = -Math.PI * 2, yMax = Math.PI * 2;
    // const xMin = -1.5, xMax = 1.5, yMin = -Math.PI * 1.01, yMax = Math.PI * 1.01;
    // const xMin = -1.5, xMax = 1.5, yMin = -Math.PI, yMax = Math.PI; // cos
    function transformX(x) { return (x - xMin) / (xMax - xMin) * width; }
    function transformY(y) { return height - ((y - yMin) / (yMax - yMin) * height); }

    // DOM elements for controls
    const inputDimInput = document.getElementById("input-dim");
    const outputDimInput = document.getElementById("output-dim");
    const hiddenSizeInput = document.getElementById("hidden-size");
    const numLayersInput = document.getElementById("num-layers");
    const learningRateInput = document.getElementById("learning-rate");
    const batchSizeInput = document.getElementById("batch-size");
    const activationSelect = document.getElementById("activation-function");
    const targetFunctionSelect = document.getElementById("target-function");
    const customFunctionInput = document.getElementById("custom-function");
    const customFunctionLabel = document.getElementById("custom-function-label");
    const resetButton = document.getElementById("reset-network");
    const pauseButton = document.getElementById("pause-training");
    const resolutionInput = document.getElementById("resolution-input");

    // Create network using vector support for input/output dimensions.
    let nn = new NeuralNetwork(
      parseInt(inputDimInput.value),
      parseInt(hiddenSizeInput.value),
      parseInt(numLayersInput.value),
      parseInt(outputDimInput.value),
      parseFloat(learningRateInput.value)
    );
    let isTraining = false;
    let batchSize = parseInt(batchSizeInput.value);
    let currentTargetFunction = "image";
    let imageData = null;
    function customListFunction(x, list) {
      if (!Array.isArray(list) || list.length === 0) {
        console.error("Provided list is not valid.");
        return 0;
      }
      // Clamp x to the range [0, list.length - 1]
      if (x <= 0) return list[0];
      if (x >= list.length - 1) return list[list.length - 1];

      // Get the lower index and the fractional part
      const index = Math.floor(x);
      const t = x - index;
      // Linear interpolation: (1-t)*value at lower index + t*value at next index
      return list[index] * (1 - t) + list[index + 1] * t;
    }
    // Target function: for output dim 1 use a scalar; if >1, cycle through input values.
    function targetFunction(x) {
      switch (currentTargetFunction) {
        case "polynomial": return x * x * (1 - x);
        case "square": return Math.sin(x) > 0 ? 1 : -1;
        case "step": return x > 0 ? 1 : -1;
        case "customList":
          // Assumes `customList` is defined elsewhere as an array of numbers.
          return customListFunction(x, customList);
        case "custom":
          try { return new Function('x', 'return ' + customFunctionInput.value)(x); }
          catch (e) { console.error("Error in custom function:", e); return Math.sin(x); }
        case "image":
          return imageData ? getImageValue(x, 0) : Math.sin(x); // For 1D input case
        case "sine":
        default: return Math.sin(x);
      }
    }

    let initialLearningRate = 0.01;  // Initial learning rate
    let decayRate = 0.0025;  // Decay rate for learning rate
    // Training step: generate a random input vector and corresponding target vector.
    function trainStep(epoch) {
      if (!isTraining) return false;
      let learningRate = initialLearningRate * Math.exp(-decayRate * epoch);
      if (learningRate < 0.00001)
        return false;
      nn.learningRate = learningRate;
      let x, y;
      if (!imageData) return false;
      const sample = sampleImagePoint();
      x = [sample.x, sample.y];
      // y = [sample.value];
      y = [sample.value[0], sample.value[1], sample.value[2]];
      nn.train(x, y, learningRate);
      return true;
    }

    // Draw function: uses first input dimension for plotting.
    function draw() {
      ctx.clearRect(0, 0, width, height);
      ctx.strokeStyle = "black";
      ctx.lineWidth = 1;
      ctx.beginPath();
      ctx.moveTo(0, transformY(0));
      ctx.lineTo(width, transformY(0));
      ctx.moveTo(transformX(0), 0);
      ctx.lineTo(transformX(0), height);
      ctx.stroke();

      if (currentTargetFunction === "image" && imageData) {
      } else {
        // Draw target function (traditional curve)
        ctx.strokeStyle = "blue";
        ctx.lineWidth = 2;
        ctx.beginPath();
        let firstPoint = true;
        for (let i = 0; i <= width; i++) {
          let xVal = xMin + (i / width) * (xMax - xMin);
          let yVal = targetFunction(xVal);
          let canvasX = transformX(xVal);
          let canvasY = transformY(yVal);
          if (firstPoint) { ctx.moveTo(canvasX, canvasY); firstPoint = false; }
          else { ctx.lineTo(canvasX, canvasY); }
        }
        ctx.stroke();
      }
      ctx.stroke();

      if (parseInt(inputDimInput.value) === 1 || currentTargetFunction !== "image") {
        // For 1D input, or non-image functions, draw as a curve
        ctx.strokeStyle = "red";
        ctx.lineWidth = 2;
        ctx.beginPath();
        let firstPoint = true;
        for (let i = 0; i <= width; i++) {
          let xVal = xMin + (i / width) * (xMax - xMin);
          let inputVec = [xVal];
          for (let j = 1; j < parseInt(inputDimInput.value); j++) { inputVec.push(0); }
          let outVec = nn.forward(inputVec);
          let canvasX = transformX(xVal);
          let canvasY = transformY(outVec[0]);
          if (firstPoint) { ctx.moveTo(canvasX, canvasY); firstPoint = false; }
          else { ctx.lineTo(canvasX, canvasY); }
        }
        ctx.stroke();
      } else if (parseInt(inputDimInput.value) === 2) {
        // For 2D input (like image data), draw a heatmap
        const resolution = parseInt(resolutionInput.value); // Number of samples in each dimension
        const cellWidth = width / resolution;
        const cellHeight = height / resolution;

        for (let iy = 0; iy < resolution; iy++) {
          for (let ix = 0; ix < resolution; ix++) {
            const x = xMin + (ix / resolution) * (xMax - xMin);
            const y = yMin + (iy / resolution) * (yMax - yMin);

            const output = nn.forward([x, y]).map(element => Math.max(0, element));
            // const output = nn.forward([x, y]);

            ctx.globalAlpha = 1.0;
            ctx.fillStyle = `rgba(${output[0] * 255},${output[1] * 255},${output[2] * 255},1)`;
            ctx.fillRect(
              ix * cellWidth,
              iy * cellHeight,
              cellWidth,
              cellHeight
            );
          }
        }
        ctx.globalAlpha = 1.0;
      }
    }

    // Update stats display
    function updateStats() {
      const statsDiv = document.getElementById("stats");
      let statsText = "Network Configuration:\n";
      statsText += `Input Dim: ${nn.inputSize}\n`;
      statsText += `Hidden Neurons: ${nn.hiddenSize}\n`;
      statsText += `Hidden Layers: ${nn.numHiddenLayers}\n`;
      statsText += `Output Dim: ${nn.outputSize}\n`;
      statsText += `Learning Rate: ${nn.learningRate}\n`;
      statsText += `Activation: ${nn.activationFunction}\n`;
      statsText += `Target Function: ${currentTargetFunction}\n\n`;

      for (let l = 0; l < nn.numHiddenLayers; l++) {
        statsText += `Layer ${l} Weights:\n` +
          nn.weights[l].map((neuron, i) => `  Neuron ${i}: ${neuron.map(w => w.toFixed(6)).join(", ")}`).join("\n") + "\n\n";
        statsText += `Layer ${l} Biases:\n` +
          nn.biases[l].map((b, i) => `  Neuron ${i}: ${b.toFixed(6)}`).join("\n") + "\n\n";
      }
      statsText += "Output Weights:\n" +
        nn.outputWeights.map((row, i) => `  Output Neuron ${i}: ${row.map(w => w.toFixed(6)).join(", ")}`).join("\n") + "\n\n";
      statsText += "Output Biases:\n" +
        nn.outputBias.map((b, i) => `  Neuron ${i}: ${b.toFixed(6)}`).join("\n");
      statsDiv.textContent = statsText;
    }

    // Generate and update the exported C function.
    // This version returns a scalar if output dim==1; otherwise it returns a vector type (vecN) with N equal to output dimension.
    function updateExport() {
      const exportArea = document.getElementById("export");
      let code = "";
      let scalar = "float";
      let actFuncName = "A";
      // switch (nn.activationFunction) {
      //     case "sigmoid":
      //         code += "// Helper sigmoid function\n" + scalar + " sigmoid(" + scalar + " x) { return 1.0 / (1.0 + exp(-x)); }\n\n";
      //         actFuncName = "sigmoid";
      //         break;
      //     case "relu":
      //         code += "// Helper ReLU function\n" + scalar + " relu(" + scalar + " x) { return x > 0 ? x : 0; }\n\n";
      //         actFuncName = "relu";
      //         break;
      //     case "leakyRelu":
      //         code += "// Helper Leaky ReLU function\n" + scalar + " leaky_relu(" + scalar + " x) { return x > 0 ? x : 0.01 * x; }\n\n";
      //         actFuncName = "leaky_relu";
      //         break;
      //     case "swish":
      //         code += "// Helper Swish function\n" + scalar + " swish(" + scalar + " x) { return x * (1.0 / (1.0 + exp(-x))); }\n\n";
      //         actFuncName = "swish";
      //         break;
      //     case "smoothstep":
      //         code += "// Helper functions for smoothstep\n";
      //         code += scalar + " mix(" + scalar + " a, " + scalar + " b, " + scalar + " t) { return (1.0 - t) * a + t * b; }\n";
      //         code += scalar + " squared(" + scalar + " x) { return x * x; }\n";
      //         code += scalar + " flip(" + scalar + " x) { return 1.0 - x; }\n";
      //         code += scalar + " smoothstart(" + scalar + " t) { return squared(t); }\n";
      //         code += scalar + " smoothstop(" + scalar + " t) { return flip(squared(flip(t))); }\n";
      //         code += scalar + " smoothstep_(" + scalar + " x) {\n";
      //         code += "    " + scalar + " t = (x + 4.0) / 8.0;\n";
      //         code += "    t = t < 0.0 ? 0.0 : (t > 1.0 ? 1.0 : t);\n";
      //         code += "    " + scalar + " result = mix(smoothstart(t), smoothstop(t), t);\n";
      //         code += "    return result * 2.0 - 1.0;\n";
      //         code += "}\n\n";
      //         actFuncName = "smoothstep_";
      //         break;
      //     case "cos":
      //         actFuncName = "cos";
      //         break;
      //     case "sin":
      //         actFuncName = "sin";
      //         break;
      //     case "sin30":
      //         actFuncName = "sin30";
      //         break;
      //     case "gelu":
      //         code += "// GELU activation function approximation\n";
      //         code += scalar + " gelu(" + scalar + " x) { return 0.5 * x * (1.0 + tanh(sqrt(2.0 / 3.141593) * (x + 0.044715 * x * x * x))); }\n\n";
      //         actFuncName = "gelu";
      //         break;
      //     case "elu":
      //         code += "// ELU activation function (alpha=1)\n";
      //         code += scalar + " elu(" + scalar + " x) { return x >= 0.0 ? x : (exp(x) - 1.0); }\n\n";
      //         actFuncName = "elu";
      //         break;
      //     case "softplus":
      //         code += "// Softplus activation function\n";
      //         code += scalar + " softplus(" + scalar + " x) { return log(1.0 + exp(x)); }\n\n";
      //         actFuncName = "softplus";
      //         break;
      //     case "linear":
      //         code += "// Linear activation function (identity)\n";
      //         code += scalar + " linear(" + scalar + " x) { return x; }\n\n";
      //         actFuncName = "linear";
      //         break;
      //     case "rayleigh":
      //         code += "// Rayleighâ€‘Jeans activation: returns x^2 for x>0, else 0\n";
      //         code += scalar + " rayleigh(" + scalar + " x) { return x > 0.0 ? x * x : 0.0; }\n\n";
      //         actFuncName = "rayleigh";
      //         break;
      //     case "expdecay":
      //         code += "// Exponential decay activation function\n";
      //         code += scalar + " expdecay(" + scalar + " x) { return exp(-x); }\n\n";
      //         actFuncName = "expdecay";
      //         break;
      //     case "tanh":
      //     default:
      //         actFuncName = "tanh";
      // }

      /*
      #define Q8(X) ((float(X) / 255.0) * 2.0 - 1.0)
      #define Q1(X) ((float(X)) * 2.0 - 1.0)
      #define Q(X) Q8(X)
      //#define Q(X) X
      #define A(X) cos(X)
      #define PI (355./113.)
      //#define PI (3.14159265359)
      
      void mainImage(out vec4 fragColor, in vec2 fragCoord) {
          vec2 uv = fragCoord / iResolution.xy;
          
          float x = mix(-PI, PI, uv.x);
          float y = mix(-PI, PI, 1. - uv.y);
          float scale = 1.0 + 1.125 * (sin(iTime) + 1.0);
          vec3 color = f(vec2(x * scale, y * scale));
          fragColor = vec4(color, 1.0);
      }
      */
      // 8-bits
      function quantize8(weight) {
        return Math.round(((weight + 1) / 2) * 255);
      }
      // 1-bit
      function quantize1(weight) {
        return weight < 0 ? 0 : 1;
      }
      // 1-bit (rounded)
      function quantize1m(weight) {
        return Math.round((weight + 1) / 2);
      }
      function quantize(weight) {
        return quantize8(weight);
      }
      // float dequantize(float quantized) {
      //     return (quantized / 127.5) - 1.0;
      // }

      //const float QUANT_SCALE = 127.0;
      //const float DEQUANT_SCALE = 1.0 / QUANT_SCALE;

      // function serialize_coeff(x)
      // Don't quantize biases
      function serialize_weight(x) {
        return quantize(x);
        // return x.toFixed(6);
      }

      // Determine return type based on output dimension.
      let retType = (nn.outputSize === 1) ? scalar : "vec" + nn.outputSize;

      // For input, if 1D then use scalar, else keep using pointer notation.
      let funcSignature = "";
      if (nn.inputSize === 1) {
        funcSignature = retType + " f(" + scalar + " x) {\n";
      } else {
        // funcSignature = retType + " f(const " + scalar + " *x) {\n";
        funcSignature = retType + " f(vec" + nn.inputSize + " x) {\n";
      }
      code += funcSignature;
      const components = [".x", ".y", ".z", ".w"];
      const tempVars = [
        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', /*'x',*/ 'y', 'z',
        'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az',
        'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm', 'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz',
        'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz',
        'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm', 'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz',
        'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez',
        'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm', 'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz',
        'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm', 'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz',
        'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm', 'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz',
        'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iu', 'iv', 'iw', 'ix', 'iy', 'iz'
      ];
      // Assume tempVars is an array of 256 short variable names.
      let tempIndex = 0;
      const hiddenLayerVars = [];
      // Preassign names for all hidden layer neurons.
      for (let l = 0; l < nn.numHiddenLayers; l++) {
        hiddenLayerVars[l] = [];
        for (let i = 0; i < nn.hiddenSize; i++) {
          hiddenLayerVars[l][i] = tempVars[tempIndex++];
        }
      }
      const outputVars = [];
      // Preassign names for all output neurons.
      for (let i = 0; i < nn.outputSize; i++) {
        outputVars[i] = tempVars[tempIndex++];
      }

      // Generate hidden layer computations.
      for (let l = 0; l < nn.numHiddenLayers; l++) {
        for (let i = 0; i < nn.hiddenSize; i++) {
          let line = "";
          if (l === 0) {
            if (nn.inputSize === 1) {
              line = "    " + scalar + " " + hiddenLayerVars[l][i] + " = " + actFuncName +
                "( x * Q(" + serialize_weight(nn.weights[0][i][0]) + ") + " +
                nn.biases[0][i].toFixed(6) + " );\n";
            } else {
              let terms = [];
              for (let j = 0; j < nn.inputSize; j++) {
                terms.push("x" + components[j] + " * Q(" + serialize_weight(nn.weights[0][i][j]) + ")");
              }
              line = "    " + scalar + " " + hiddenLayerVars[l][i] + " = " + actFuncName +
                "( " + terms.join(" + ") + " + " + nn.biases[0][i].toFixed(6) + " );\n";
            }
          } else {
            let terms = [];
            for (let j = 0; j < nn.hiddenSize; j++) {
              terms.push(hiddenLayerVars[l - 1][j] + " * Q(" + serialize_weight(nn.weights[l][i][j]) + ")");
            }
            line = "    " + scalar + " " + hiddenLayerVars[l][i] + " = " + actFuncName +
              "( " + terms.join(" + ") + " + " + nn.biases[l][i].toFixed(6) + " );\n";
          }
          code += line;
        }
      }

      // Compute output neurons.
      for (let i = 0; i < nn.outputSize; i++) {
        let line = "";
        let last = nn.numHiddenLayers - 1;
        let terms = [];
        for (let j = 0; j < nn.hiddenSize; j++) {
          terms.push(hiddenLayerVars[last][j] + " * Q(" + serialize_weight(nn.outputWeights[i][j]) + ")");
        }
        line = "    " + scalar + " " + outputVars[i] + " = " + nn.outputBias[i].toFixed(6) +
          " + " + terms.join(" + ") + ";\n";
        code += line;
      }

      // Return the output using vector constructor if output dim > 1.
      if (nn.outputSize === 1) {
        code += "    return " + outputVars[0] + ";\n";
      } else {
        code += "    return " + retType + "(" + outputVars.join(", ") + ");\n";
      }
      code += "}\n";

      // // Generate hidden layer computations
      // for (let l = 0; l < nn.numHiddenLayers; l++) {
      //     for (let i = 0; i < nn.hiddenSize; i++) {
      //         let line = "";
      //         if (l === 0) {
      //             if (nn.inputSize === 1) {
      //                 line = "    " + scalar + " a0_" + i + " = " + actFuncName + "( x * Q(" + serialize_weight(nn.weights[0][i][0]) + ") + " + nn.biases[0][i].toFixed(6) + " );\n";
      //             } else {
      //                 let terms = [];
      //                 for (let j = 0; j < nn.inputSize; j++) {
      //                     terms.push("x" + components[j] + " * Q(" + serialize_weight(nn.weights[0][i][j]) + ")");
      //                 }
      //                 line = "    " + scalar + " a0_" + i + " = " + actFuncName + "( " + terms.join(" + ") + " + " + nn.biases[0][i].toFixed(6) + " );\n";
      //             }
      //         } else {
      //             let terms = [];
      //             for (let j = 0; j < nn.hiddenSize; j++) {
      //                 terms.push("a" + (l - 1) + "_" + j + " * Q(" + serialize_weight(nn.weights[l][i][j]) + ")");
      //             }
      //             line = "    " + scalar + " a" + l + "_" + i + " = " + actFuncName + "( " + terms.join(" + ") + " + " + nn.biases[l][i].toFixed(6) + " );\n";
      //         }
      //         code += line;
      //     }
      // }

      // // Compute output neurons.
      // for (let i = 0; i < nn.outputSize; i++) {
      //     let line = "";
      //     let last = nn.numHiddenLayers - 1;
      //     let terms = [];
      //     for (let j = 0; j < nn.hiddenSize; j++) {
      //         terms.push("a" + last + "_" + j + " * Q(" + serialize_weight(nn.outputWeights[i][j]) + ")");
      //     }
      //     line = "    " + scalar + " out" + i + " = " + nn.outputBias[i].toFixed(6) + " + " + terms.join(" + ") + ";\n";
      //     code += line;
      // }

      // Return the output using vector constructor if output dim > 1.
      // if (nn.outputSize === 1) {
      //     code += "    return out0;\n";
      // } else {
      //     let outs = [];
      //     for (let i = 0; i < nn.outputSize; i++) {
      //         outs.push("out" + i);
      //     }
      //     code += "    return " + retType + "(" + outs.join(", ") + ");\n";
      // }
      // code += "}\n";

      // const values = nn.toFlatArray();
      // code += `const float coeff[${values.length+1}] = float[${values.length+1}](`;
      // for (let v of values)
      // {
      //     code += `${v.toFixed(6)},`;
      // }
      // code += `0.);`;
      /*

      const int INPUT_SIZE  = 2;
      const int HIDDEN_SIZE = 20;
      const int NUM_HIDDEN  = 4;
      const int OUTPUT_SIZE = 3;

      // Activation function (assumed defined elsewhere)
      float activation(float x) {
          // e.g., a ReLU or sigmoid â€“ adjust as needed.
          return max(0.0, x);
      }

      // Neural network function
      // Using a vector input (for INPUT_SIZE > 1):
      vec1 f(vec3 x) {
          // Compute first hidden layer:
          float a0[HIDDEN_SIZE];
          for (int i = 0; i < HIDDEN_SIZE; i++) {
              // dot product of x and weights0[i]
              float sum = biases0[i];
              for (int j = 0; j < INPUT_SIZE; j++) {
                  sum += x[j] * weights0[i][j];
              }
              a0[i] = activation(sum);
          }
          
          // Compute second (and subsequent) hidden layers:
          float a1[HIDDEN_SIZE];
          for (int i = 0; i < HIDDEN_SIZE; i++) {
              float sum = biases1[i];
              for (int j = 0; j < HIDDEN_SIZE; j++) {
                  sum += a0[j] * weights1[i][j];
              }
              a1[i] = activation(sum);
          }
          
          // Compute output layer:
          float out[OUTPUT_SIZE];
          for (int i = 0; i < OUTPUT_SIZE; i++) {
              float sum = outputBiases[i];
              for (int j = 0; j < HIDDEN_SIZE; j++) {
                  sum += a1[j] * outputWeights[i][j];
              }
              out[i] = sum;
          }
          
          // Return a scalar if OUTPUT_SIZE == 1.
          // (If OUTPUT_SIZE > 1, you could return a vector constructed from out[0], out[1], etc.)
          return out[0];
      }
      */

      exportArea.value = code;

    }

    // Event listeners
    activationSelect.addEventListener("change", function () { nn.setActivationFunction(this.value); });
    targetFunctionSelect.addEventListener("change", function () {
      currentTargetFunction = this.value;
      if (this.value === "custom") {
        customFunctionInput.style.display = "inline-block";
        customFunctionLabel.style.display = "inline-block";
      } else {
        customFunctionInput.style.display = "none";
        customFunctionLabel.style.display = "none";
      }
    });
    customFunctionInput.addEventListener("change", function () {
      try { new Function('x', 'return ' + this.value)(0); }
      catch (e) { alert("Invalid function expression. Please check your syntax."); this.value = "Math.sin(x)"; }
    });
    inputDimInput.addEventListener("change", function () {
      const val = parseInt(this.value);
      if (val > 0) { nn.inputSize = val; nn.initializeWeights(); }
      else { this.value = "1"; }
    });
    outputDimInput.addEventListener("change", function () {
      const val = parseInt(this.value);
      if (val > 0) { nn.outputSize = val; nn.initializeWeights(); }
      else { this.value = "1"; }
    });
    hiddenSizeInput.addEventListener("change", function () {
      const size = parseInt(this.value);
      if (size > 0) { nn.setHiddenSize(size); }
      else { this.value = "1"; nn.setHiddenSize(1); }
    });
    numLayersInput.addEventListener("change", function () {
      const num = parseInt(this.value);
      if (num > 0) { nn.setNumHiddenLayers(num); }
      else { this.value = "1"; nn.setNumHiddenLayers(1); }
    });
    learningRateInput.addEventListener("change", function () {
      const rate = parseFloat(this.value);
      if (rate > 0) { nn.setLearningRate(rate); }
      else { this.value = "0.01"; nn.setLearningRate(0.01); }
    });
    batchSizeInput.addEventListener("change", function () {
      const size = parseInt(this.value);
      if (size > 0) { batchSize = size; }
      else { this.value = "100"; batchSize = 100; }
    });
    resetButton.addEventListener("click", function () { nn.initializeWeights(); });
    pauseButton.addEventListener("click", function () {
      isTraining = !isTraining;
      this.textContent = isTraining ? "Pause Training" : "Resume Training";
    });
    const imageUploadContainer = document.getElementById('image-upload-container');
    const imageUploadInput = document.getElementById('image-upload');
    const imageScaleInput = document.getElementById('image-scale');
    const scaleValueSpan = document.getElementById('scale-value');
    const useImageButton = document.getElementById('use-image');

    // Update the scale value display when slider changes
    imageScaleInput.addEventListener('input', () => {
      scaleValueSpan.textContent = imageScaleInput.value;

      // If we already have an image loaded, update the preview
      if (imageUploadInput.files && imageUploadInput.files[0]) {
        processSelectedImage();
      }
    });

    // Handle image upload change
    imageUploadInput.addEventListener('change', processSelectedImage);

    function processSelectedImage() {
      const file = imageUploadInput.files[0];
      if (!file) return;

      const reader = new FileReader();
      reader.onload = function (e) {
        const img = new Image();
        img.onload = function () {
          const scale = parseFloat(imageScaleInput.value);
          const processed = processImage(img, scale);

          // Store but don't activate until "Use This Image" is clicked
          window.tempImageData = processed;
        };
        img.src = e.target.result;
      };
      reader.readAsDataURL(file);
    }

    // Use the image when the button is clicked
    useImageButton.addEventListener('click', () => {
      if (window.tempImageData) {
        imageData = window.tempImageData;

        // Set appropriate dimensions for image data
        inputDimInput.value = "2";
        nn.inputSize = 2;

        outputDimInput.value = "3";
        nn.outputSize = 3;
        isTraining = true;

        // Re-initialize the network with the new dimensions
        nn.initializeWeights();

        // Update interface
        // alert("Neural network dimensions set to 2D input, 1D output for image processing.");
      } else {
        alert("Please upload an image first.");
      }
    });
    function setTargetFunctionSelect(val) {
      currentTargetFunction = val;
      if (val === "custom") {
        customFunctionInput.style.display = "inline-block";
        customFunctionLabel.style.display = "inline-block";
        imageUploadContainer.style.display = "none";
      } else if (val === "image") {
        customFunctionInput.style.display = "none";
        customFunctionLabel.style.display = "none";
        imageUploadContainer.style.display = "flex";
      } else {
        customFunctionInput.style.display = "none";
        customFunctionLabel.style.display = "none";
        imageUploadContainer.style.display = "none";
      }
    }
    // Show/hide image upload container based on selected function
    targetFunctionSelect.addEventListener("change", function () {
      setTargetFunctionSelect(this.value);
    });
    function animate() {
      for (let i = 0; i < batchSize; i++) { if (!trainStep(epoch_)) break; }
      draw();
      updateStats();
      updateExport();
      epoch_++;
      requestAnimationFrame(animate);
    }
    animate();
    window.onload = () => { setTargetFunctionSelect("image"); };
  </script>
</body>

</html>